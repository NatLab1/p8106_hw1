---
title: "HW 1"
author: "Nathalie Fadel"
date: "2/19/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(boot)
library(tidyverse)

```

##Import data
```{r}
sol_test = read_csv("data/solubility_test.csv") #test data

sol_train = read_csv("data/solubility_train.csv") #training data
```

##Part A: Model fitting & cross-validation
```{r}
ctrl1 <- trainControl(method = "cv", number = 10) #10-fold cv
ctrl2 <- trainControl(method = "LOOCV") #leave others out cv
ctrl3 <- trainControl(method = "none") # only fits one model to the entire training set
ctrl4 <- trainControl(method = "boot632") #bootstrap method
ctrl5 <- trainControl(method = "repeatedcv", repeats = 5) #5 times to repeat cv
ctrl6 <- trainControl(method = "LGOCV") #leave group out cv, p = 0.75 is default, number = 25 is default

sol_train <- na.omit(sol_train)

sol_test <- na.omit(sol_test)

set.seed(1)
sol_fit1 <- train(Solubility~., 
                data = sol_train, 
                method = "lm", 
                trControl = ctrl3)
summary(sol_fit1)

sol_fit2 <- train(Solubility~., 
                data = sol_test, 
                method = "lm", 
                trControl = ctrl2) #LOOCV 
sol_fit2 #RMSE 1.561272, R^2 0.6137344, MAE 1.02854

```

The model that was fit used all of the possible variables (228) in the sol_train dataset, with Solubility as the outcome variable. Then, using the Monte-Carlo method, also known as Leave One Out Cross-Validation, on the sol_test dataset, the RMSE was found to be 1.561272, R-squared is 0.6137344, and MAE is 1.02854.

##Part B: Ridge Regression 
```{r}


```






