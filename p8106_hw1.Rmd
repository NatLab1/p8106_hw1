---
title: "HW 1"
author: "Nathalie Fadel"
date: "2/19/2019"
output: github_document
---

```{r setup, include=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(boot)
library(tidyverse)
library(ISLR)
library(glmnet)
library(corrplot)
library(plotmo)
library(glmnetUtils)
library(pls)
```

##Import data
```{r}
sol_test = read_csv("data/solubility_test.csv") #test data

sol_train = read_csv("data/solubility_train.csv") #training data

#Omit NA values
sol_train <- na.omit(sol_train)

sol_test <- na.omit(sol_test)
```

##Part A: Linear model fitting & finding MSE
```{r}
set.seed(1)
sol_fit1 = lm(Solubility~., data = sol_train)

summary(sol_fit1)

sol_fit2 = predict(sol_fit1, data = sol_test)

MSE=mean((sol_test$Solubility-sol_fit2)^2)
#Show MSE
print(MSE)

```
The model that was fit used all of the possible variables (228) in the sol_train dataset, with Solubility as the outcome variable. MSE found using test data is 8.915445. 

##Part B: Ridge Regression 
```{r}
# train data input matrix and response vector
xtrain = model.matrix(Solubility~.,sol_train)[,-1]
ytrain = sol_train$Solubility
#test data input matrix and response vector
xtest = model.matrix(Solubility~.,sol_test)[,-1]
ytest = sol_test$Solubility

#Using cross-validation to choose the tuning parameter 
set.seed (1)
cv.out = cv.glmnet(xtrain, ytrain, alpha=0)
plot(cv.out)
```

```{r}
bestlam1 = cv.out$lambda.min
#Creating training model using ridge regression
model1 = glmnet(xtrain, ytrain, alpha=0, lambda=bestlam1)
#Printing out the logistic model
model1$beta

#Fitting training model on test set
pred1 = predict(model1, s=bestlam1, newx=xtest)
#Calculating Accuracy
MSE1 = mean((pred1-ytest)^2)
print(MSE1)

```
The MSE from the ridge regression model is 0.5559241.

##Part C: Lasso
```{r}
set.seed (1)
cv.out2 = cv.glmnet(xtrain, ytrain, alpha = 1)
plot(cv.out2)
```

```{r}
bestlam2 = cv.out2$lambda.min
#Creating training model using lasso regression
model2 = glmnet(xtrain, ytrain, alpha=1, lambda=bestlam2)
#Printing out the logistic model
model2$beta
```
There are `r (228-87)` nonzero predictors in the model.   

```{r}
#Fitting training model on test set
pred2 = predict(model2, s=bestlam2, newx=xtest)
#Calculating Accuracy
MSE2=mean((pred2-ytest)^2)
#Printing MSE
print(MSE2)
```
The MSE of the lasso model is 0.4952673. 

```{r}
#Retrieving the lasso coefficients
lasscoef=predict(model2, type="coefficients", s=bestlam2)[1:length(model2$beta),]
#Printing non zero coefficients
lasscoef[lasscoef!=0]
```

##Part D: PCR Model
```{r}
set.seed(1)
pcr.fit = pcr(Solubility~., data = sol_train, scale=TRUE, validation="CV")
summary(pcr.fit)
```

```{r}
validationplot(pcr.fit, val.type="MSEP")
```

```{r}
pcr.pred = predict(pcr.fit, newdata = sol_test, ncomp=228)
mean((pcr.pred-ytest)^2)
```  
The MSE is 0.5558898, and the value of M is 228.  

##Part E: Conclusion  
The Mean Squared Error from the Lasso regression model, 0.4952673, is the lowest out of the four models tested. Therefore, we should use this as the prediction model for Solubility. 
