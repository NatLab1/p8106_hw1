---
title: "HW 1"
author: "Nathalie Fadel"
date: "2/19/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(boot)
library(tidyverse)
library(ISLR)
library(glmnet)
library(corrplot)
library(plotmo)
library(glmnetUtils)
```

##Import data
```{r}
sol_test = read_csv("data/solubility_test.csv") #test data

sol_train = read_csv("data/solubility_train.csv") #training data

#Omit NA values
sol_train <- na.omit(sol_train)

sol_test <- na.omit(sol_test)
```

##Part A: Linear model fitting & finding MSE
```{r}
set.seed(1)
sol_fit1 = lm(Solubility~., data = sol_train)

summary(sol_fit1)

sol_fit2 = predict(sol_fit1, data = sol_test)

MSE=mean((sol_test$Solubility-sol_fit2)^2)
#Show MSE
print(MSE)

```
The model that was fit used all of the possible variables (228) in the sol_train dataset, with Solubility as the outcome variable. MSE found using test data is 8.915445. 

##Part B: Ridge Regression 
```{r}
# train data input matrix and response vector
xtrain = model.matrix(Solubility~.,sol_train)[,-1]
ytrain = sol_train$Solubility
#test data input matrix and response vector
xtest = model.matrix(Solubility~.,sol_test)[,-1]
ytest = sol_test$Solubility

#Using cross-validation to choose the tuning parameter ??
set.seed (1)
cv.out = cv.glmnet(xtrain, ytrain, alpha=0)
plot(cv.out)
```

```{r}
bestlam = cv.out$lambda.min
#Creating training model using ridge regression
model = glmnet(xtrain, ytrain, alpha=0, lambda=bestlam)
#Printing out the logistic model
model$beta

#Fitting training model on test set
pred = predict(model, s=bestlam, newx=xtest)
#Calculating Accuracy
MSE = mean((pred-ytest)^2)
print(MSE)

```

##Part C: Lasso
```{r}
cv.lasso <- cv.glmnet(x,y, alpha = 1, lambda = exp(seq(-1, 5, length=100))) #want lambda values that give smallest MSE. Make sure you have global min, not local min.
#default value is standardize = TRUE
cv.lasso$lambda.min

plot(cv.lasso)
```

```{r}
predict(cv.lasso, s="lambda.min", type="coefficients")
#Results: Intercept, FP044, FP072, FP089, FP172, MolWeight, NumCarbon, HydrophilicFactor, SurfaceArea1

set.seed(2)
lasso.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-1, 5, length=100))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)
plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune

coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```