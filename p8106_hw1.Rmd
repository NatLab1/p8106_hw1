---
title: "HW 1"
author: "Nathalie Fadel"
date: "2/19/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(boot)
library(tidyverse)
library(ISLR)
library(glmnet)
library(corrplot)
library(plotmo)
library(glmnetUtils)
```

##Import data
```{r}
sol_test = read_csv("data/solubility_test.csv") #test data

sol_train = read_csv("data/solubility_train.csv") #training data

#Omit NA values
sol_train <- na.omit(sol_train)

sol_test <- na.omit(sol_test)
```

##Part A: Linear model fitting & finding MSE
```{r}
set.seed(1)
sol_fit1 = lm(Solubility~., data = sol_train)

summary(sol_fit1)

sol_fit2 = predict(sol_fit1, data = sol_test)

MSE=mean((sol_test$Solubility-sol_fit2)^2)
#Show MSE
print(MSE)

```
The model that was fit used all of the possible variables (228) in the sol_train dataset, with Solubility as the outcome variable. MSE found using test data is 8.915445. 

##Part B: Ridge Regression 
```{r}

# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Solubility~.,sol_train)[,-1]
# vector of response
y <- sol_train$Solubility

corrplot(cor(x))
```

```{r}
# fit the ridge regression (alpha = 0) with a sequence of lambdas
grid = 10^seq (-1, 10, length = 100)
ridge.mod <- glmnet(x, y, alpha=0, lambda = grid) #sequence from -1 to 10
#use exp from negative 1 to small positive number so you can use small tuning parameter
#default setting in glmnet is standardize = TRUE.

mat.coef <- coef(ridge.mod)
dim(mat.coef)

#cross-validation
set.seed(2)
cv.ridge <- cv.glmnet(x, y, 
                      alpha = 0, 
                      lambda = grid, 
                      type.measure = "mse")

plot(cv.ridge)
```

```{r}
plot(ridge.mod, xvar = "lambda", label = TRUE)
#Or:
plot_glmnet(ridge.mod, xvar = "rlambda")
```

```{r}
#coefficients of final model
best.lambda <- cv.ridge$lambda.lse #lambda.min will give lambda with smallest MSE, .lse will do reverse
best.lambda

predict(ridge.mod, s = best.lambda, newx = x , type="response")
```

##Part C: Lasso
```{r}
cv.lasso <- cv.glmnet(x,y, alpha = 1, lambda = exp(seq(-1, 5, length=100))) #want lambda values that give smallest MSE. Make sure you have global min, not local min.
#default value is standardize = TRUE
cv.lasso$lambda.min

plot(cv.lasso)
```

```{r}
predict(cv.lasso, s="lambda.min", type="coefficients")
#Results: Intercept, FP044, FP072, FP089, FP172, MolWeight, NumCarbon, HydrophilicFactor, SurfaceArea1

set.seed(2)
lasso.fit <- train(x, y,
                     method = "glmnet",
                     tuneGrid = expand.grid(alpha = 1, 
                                            lambda = exp(seq(-1, 5, length=100))),
                   # preProc = c("center", "scale"),
                     trControl = ctrl1)
plot(lasso.fit, xTrans = function(x) log(x))

lasso.fit$bestTune

coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```